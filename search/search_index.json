{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Making eDNA FAIR","text":"This is an ongoing, community-led initiative, and we continue to refine and improve the products on this site based on feedback and collaboration. Check out the Our next steps page to learn more about how we are working with the Genomic Standards Consortium (GSC) and the Biodiversity Information Standards (TDWG)."},{"location":"index.html#why-do-we-need-fair-edna","title":"Why do we need FAIR eDNA?","text":"<p>The success of environmental DNA (eDNA) approaches for species detection has revolutionised biodiversity monitoring and distribution mapping. Targeted eDNA amplification approaches, such as quantitative PCR, have improved our understanding of species distribution, while metabarcoding-based approaches enable biodiversity assessment at unprecedented scales and taxonomic resolution. eDNA datasets, however, are often scattered across repositories with inconsistent formats, varying access restrictions, and inadequate metadata; this limits their interoperation, reuse, and overall impact. Adopting FAIR (Findable, Accessible, Interoperable, Reusable) data practices with eDNA data can transform monitoring of biodiversity and individual species, and support data-driven biodiversity management across broad scales. FAIR practices remain underdeveloped in the eDNA community, partly due to gaps in adapting existing vocabularies, such as Darwin Core (DwC) and Minimum Information about any (x) Sequence (MIxS), to eDNA-specific needs and workflows. </p>"},{"location":"index.html#project-scope","title":"Project scope","text":"<p>Fine-tuning, optimizing and extending existing data and metadata standards specifically for eDNA-based studies is a crucial first step towards achieving FAIR eDNA. To this end, we propose a comprehensive FAIR eDNA (FAIRe) metadata checklist, which integrates existing data standards and introduces new terms tailored to eDNA workflows.  Metadata are systematically linked to both raw data (e.g., metabarcoding sequences, Ct/Cq values of targeted qPCR assays) and derived biological observations (e.g., ASV/OTU tables, species presence/absence). Along with formatting guidelines, tools, templates and example datasets, we introduce a standardised, ready-to-use approach for FAIR eDNA practices.  Together, these efforts aim to guide data providers and facilitate the unambiguous description of eDNA datasets that are aligned with FAIR data practices. Our goal is to provide a FAIRe metadata checklist and guidelines as resources for existing data standards, infrastructures and databases, helping them to address gaps to better meet their own objectives. The guidelines will also benefit individual eDNA science practitioners and labs by improving data management standardisation, making their data more FAIR not only for reusers, but also for internal organisation.</p> Designed by: OOID Scientific"},{"location":"citation.html","title":"How to cite the FAIRe guidelines","text":"<p>If you use the FAIRe guidelines, please cite the following manuscript:</p> <p>Takahashi, M., Fr\u00f8slev, T.G., Paup\u00e9rio, J., Thalinger, B., Klymus, K., Helbing, C.C., Villacorta-Rath, C., Silliman, K., Thompson, L.R., Jungbluth, S.P., Yong, S.Y., Formel, S., Jenkins, G., Laporte, M., Deagle, B., Rajbhandari, S., Jeppesen, T.S., Bissett, A., Jerde, G., Hahn, E., Schriml, L.M., Hunter, C., Newman, P., Woollard, P., Harper, L.R., Dunn, N., West, K., Haderl\u00e9, R., Wilkinson, S., Acharya-Patel, N., Lopez, M.L.D., Cochrane, G., Berry, O. (2025) A metadata checklist and data formatting guidelines to make eDNA FAIR (Findable, Accessible, Interoperable and Reusable). Environmental DNA. https://doi.org/10.1002/edn3.70100</p> <p>Stay tuned for updates and improvements here on the FAIRe website!</p>"},{"location":"contact.html","title":"How to contribute","text":"<p>Your feedback is essential for keeping the FAIRe metadata checklist and formatting guidelines useful and up to date!</p> <p>We maintain several repositories for the guidelines, checklist, and scripts. Please raise issues in the repository most relevant to your topic, or use FAIReDNA.github.io for any general FAIRe-related matters. We actively monitor all repositories and will respond as quickly as possible.</p> <ul> <li>Checklist: https://github.com/FAIR-eDNA/FAIRe_checklist/issues</li> <li>FAIRe-ator: https://github.com/FAIR-eDNA/FAIRe-ator/issues</li> <li>FAIRe2MDT: https://github.com/FAIR-eDNA/FAIRe2MDT/issues</li> <li>General &amp; other FAIRe guideline issues: https://github.com/FAIR-eDNA/FAIR-eDNA.github.io/issues</li> </ul> <p>Alternatively, please email to miwa.takahashi@csiro.au to provide any feedback.</p>"},{"location":"download.html","title":"Download","text":""},{"location":"download.html#faire-metadata-checklist","title":"FAIRe metadata checklist","text":"<p>The latest version of the FAIRe metadata checklist (v1.0.2 released on 2025-07-02) is available in Excel format here You can access previous versions and change logs here.</p>"},{"location":"download.html#faire-template","title":"FAIRe template","text":"<p>Click here to download the FAIRe FULL template (v1.0.2). </p> <p>Note: This template includes all terms from the FAIRe checklist, including those specifically tailored for certain assay types (i.e., targeted and metabarcoding) and sample types (e.g., water, sediment). Costumised template based on your assay type and sample type can be produced using the R code, FAIR eDNA template generator (FAIRe-ator). </p>"},{"location":"download.html#codes-and-tools","title":"Codes and tools","text":"<ul> <li>FAIRe-ator (FAIR eDNA template generator): Download the README and R code here.</li> <li>FAIReSheets (FAIR eDNA template generator for Google Sheets): Download the README and Python code here.</li> <li>FAIRe2MDT: Download the R code here.</li> <li>FAIRe-fier (FAIR eDNA metadata verifier): This tool is accessible via web interface, with additional metadata and citation details available at http://hdl.handle.net/102.100.100/706519?index=1.</li> </ul>"},{"location":"download.html#example-datasets","title":"Example datasets","text":"<p>A variety of example datasets from published metabarcoding and targeted assay studies is available here.</p>"},{"location":"guidelines.html","title":"FAIR eDNA (FAIRe) guidelines","text":"<p>In this section, we outline the various data components and formats, present a comprehensive FAIR eDNA (FAIRe) metadata checklist (click here to download) along with scripts and tools to guide users through the data formatting process. </p> Figure 2. FAIRe practice flowchart"},{"location":"guidelines.html#data-formats-and-components","title":"Data formats and components","text":"<p>The figure and list below outline the associated eDNA data components as well as their contents and purposes. In our proposed data structure, all data components, except DNA sequence data (i.e., raw data in FASTQ and, optionally, representative sequences in FASTA), are organised in a tabular format. They can be saved as individual worksheets in spreadsheet workbooks, as character-separated values text files (tab or comma), or as a combination of these (see various example datasets provided). This allows data to be conceptually divided into separate modules that can be linked using unique identifiers, such as <code>project_id</code>, <code>sample_name</code>, and <code>assay_name</code>, as well as to accommodate users\u2019 needs and data wrangling preferences. </p> Figure 3. Data components and their recommended structures that promote FAIR eDNA <ol> <li> <p>Project metadata (projectMetadata)</p> <ul> <li>Contents: Information that applies to an entire project and dataset. This includes a project name, reference associated to the datasets from which the datasets were derived (e.g., bibliographic references to studies, DOI of published, associated data), and metadata describing workflows including PCR, sequencing, and bioinformatic steps.</li> <li>Purpose: To describe project information and methodologies that apply to all samples, thereby avoiding the need to propagate identical entries to all samples in sample metadata. Information, such as bioinformatic quality filtering parameters and thresholds, enable data reusers to assess whether the data quality meets the specific requirements for certain types of reuse. Standardisation of such information is particularly important as minimum requirements for amplification analysis, bioinformatic workflows and quality assurance levels vary between studies and applications, including those reusing shared data. </li> <li>Note: Multiple primer sets are often applied within one project to target more than one taxon. In this case, practitioners would follow the format in Figure 5 to record assay-specific workflows and outputs.</li> </ul> </li> <li> <p>Sample metadata (sampleMetadata)</p> <ul> <li>Contents: Information for each sample (where this cannot be stated at the project level in project metadata), including sampling date, location, methodologies, and environmental variables (i.e., temperature, water depth).</li> <li>Purpose: To record sample-specific information through the workflows from sampling to bioinformatics. </li> </ul> </li> <li> <p>Experiment/run metadata (experimentRunMetadata)</p> <ul> <li>Contents: Sample-specific information for PCR, library preparation and sequencing workflows.</li> <li>Purpose: To record workflows, from PCR onward, that are specific to each sample and assay.</li> <li>Note: Each entry (row) has a unique library ID (<code>lib_id</code>) and multiplexing identifiers (MIDs) (<code>mid_forward</code>, <code>mid_reverse</code>).</li> </ul> </li> <li> <p>PCR standard data (stdData)</p> <ul> <li>Contents: Detailed information of PCR standards, including starting input quantity, Ct/Cq values, and standard curve parameters (e.g., efficiency, R2).</li> <li>Purpose: To improve reproducibility and allow data reusers to regenerate the estimated copy numbers using their own approach. The entries in <code>assay_name</code> and <code>pcr_plate_id</code> in the PCR standard data and amplification data allow linking of the standards and eDNA samples.</li> </ul> </li> <li> <p>eLow Quant data (eLowQuantData)</p> <ul> <li>Contents: Outputs from the eLow Quant binomial approach for PCR standards if applicable (see Lesperance et al., 2021).</li> <li>Purpose: To improve reproducibility and allow data reusers to regenerate the estimated copy numbers using their own approach.</li> </ul> </li> <li> <p>Targeted assay amplification data (ampData)</p> <ul> <li>Contents: Raw amplification data of each PCR technical replicate performed on eDNA samples, as well as estimated copy numbers based on standard curves (if applicable) and detection status of targeted taxon in each biological replicate (field sample).</li> <li>Purpose: The Ct/Cq values of each technical replicate, combined with the standard information above, enable the re-conversion of the raw amplification data to concentration or detection status in future studies. Detect/non-detect records of a target taxon at a biological replicate level allow prompt reuses of taxon occurrence records in future studies, combined with sample metadata. They also standardise eDNA data in the context of biodiversity data produced using other methods, and consequently make the collective data and underlying research discoverable and reusable for wider audiences. </li> <li>Note: \"NA\" (not zero) should be used under the term <code>quantificationCycle</code> (i.e., Ct/Cq values) to indicate no amplification occurred, because zero could be interpreted as an extremely high DNA concentration, which may or may not be target DNA, that would be unquantifiable due to being outside the standard curve range.</li> </ul> </li> <li> <p>Raw DNA sequences</p> <ul> <li>Contents: DNA sequences in FASTQ format, demultiplexed for each sample. The sequences of primers, adapters and MIDs must be removed.</li> <li>Purpose: To allow data reusers to repeat quality filtering, denoising and taxonomic assignment using their own algorithms and thresholds to fit their purposes. To enable re-analyses using different or improved pipelines/algorithms. To enhance the reproducibility of studies. </li> </ul> </li> <li> <p>Non-curated ASV/OTU table (otuRaw) (Optional)</p> <ul> <li>Contents: The initial ASV/OTU table produced in a bioinformatic pipeline, containing absolute sequence read counts of all ASV/OTUs (including contaminant/non-targeted taxa and unassigned ASVs/OTUs) across all samples (including controls). Each ASV/OTU will be provided with a unique <code>seq_id</code> at this stage. </li> <li>Purpose: To communicate objective information of read counts of all OTUs/ASVs in all samples, and allow data reusers to modify or re-run the data curation steps if required. The inclusion of control samples, suspected contaminants and non-targeted taxa allows the investigations of common contamination sequences, taxa, and sources across multiple studies, and potentially identify how to minimize them.</li> <li>Note: The column names in ASV/OTU tables should be either <code>samp_name</code> or <code>lib_id</code>, making sure they maintain clear links between the experiment/run metadata and ASV/OTU table.</li> <li>Note: Although it is called a \u201craw\u201d ASV/OTU table, some level of compression, quality filtering and curation may already have been applied, such as filtering based on minimum length, read counts, error rates, and OTU clustering. The extent of data curation performed on the non-curated ASV/OTU table can vary between studies depending on the bioinformatics pipeline used, and this should be described under the term <code>otu_raw_description</code> in the project metadata. </li> </ul> </li> <li> <p>Curated ASV/OTU table (otuFinal)</p> <ul> <li>Contents: An ASV/OTU table resulting from various data curation processes, including denoising (i.e., custom or LULU curation), removing control samples, and eliminating suspected contamination and non-target taxa. This table should ideally be identical to, or correspond closely to, the one used for the final (e.g. ecological) analyses published in associated scientific papers. ASVs/OTUs without taxonomic assignments should remain in the otuFinal table if they were relevant to the analyses.</li> <li>Purpose: To facilitate reproducibility of original (ecological) analyses, and general reuse of a curated/cleaned version of the data. To improve interoperability with other types of biodiversity data, and thereby facilitate incorporation in general biodiversity databases, and make the data and underlying research discoverable and reusable for wider audiences. </li> <li>Note: The column names in ASV/OTU tables should be either <code>samp_name</code> or <code>lib_id</code>, making sure they maintain clear links between the experiment/run metadata and ASV/OTU table.</li> <li>Note: In some studies, researchers may choose to collapse multiple ASVs/OTUs assigned to the same taxon, summing their read counts. We, however, strongly encourage data providers to submit ASV/OTU tables in their original, non-collapsed form. Preserving each ASV/OTU with its individual read counts and sequence ensures better interoperability and prevents the loss of potentially valuable ASVs/OTUs due to uncertain or erroneous taxonomic annotations resulting from incomplete or imperfect reference databases at the time of analysis. </li> <li>Note: Descriptions on how data were curated to produce the curated ASV/OTU table should be recorded under the term <code>otu_final_description</code> in the project metadata.</li> </ul> </li> <li> <p>Non-curated Sequence/Taxa table (taxaRaw) (Optional)</p> <ul> <li>Contents: In addition to the contents in the curated Sequence - Taxa table (below), this table includes ASVs/OTUs that were excluded from the curated ASV/OTU table (e.g., non-target taxa, contaminants, positive control sequences, ASVs with no taxa hits). It may also contain multiple rows for a single ASV/OTU assigned to multiple taxa, with information of each assigned taxon.  </li> <li>Purpose: To enhance data transparency by providing access to ASVs, OTUs, and taxa excluded from the curated ASV/OTU table. To allow data reusers the opportunity to re-assess and re-run specific data curation steps.</li> <li>Note: When assigning a lowest common ancestor (LCA) to a single ASV/OTU, multiple rows should be generated for the ASV/OTU to capture information about each of the assigned taxa. Users may apply a similarity threshold (e.g., 97%) and include all reference sequences within this range, or a representative subsample of them, in the output.</li> <li>Note: See the example in Figure 3 (ASV1 in the non-curated Sequence/Taxa table).</li> </ul> </li> <li> <p>Curated Sequence/Taxa table (taxaFinal)</p> <ul> <li>Contents: DNA sequence of each <code>seq_id</code> listed in the curated ASV/OTU table. If a taxon was assigned, also include assigned taxonomy and assignment quality assurance parameters (e.g., % identity, % query coverage). If multiple taxa are assigned to a single ASV/OTU, the LCA should be used as the assigned taxon. </li> <li>Purpose: To allow data reusers to assess the specificity and accuracy of the inferred taxonomy, and to perform taxonomic re-annotation using different or updated sequence reference databases.</li> <li>Note: ASVs/OTUs without taxonomic assignments should be included in the taxaFinal table, but without taxonomic information (e.g., <code>scientificName</code>), if they were relevant to the study scope.</li> </ul> </li> </ol> Figure 4. Optional intermediate file formats for metabarcoding outputs Figure 5. Metadata formats for a project applying multiple assays"},{"location":"guidelines.html#consistent-identifiers-and-file-naming","title":"Consistent identifiers and file naming","text":"<p>To ensure machine readability and long-lasting reference to a digital resource, it is crucial to apply consistent, persistent sample and sequence identifiers (<code>samp_name</code>, <code>lib_id</code> and <code>seq_run_id</code>) across datasets (Damerow et al., 2021; McMurry et al., 2017). Similarly, each file must have a clear, unambiguous name, consisting of <code>project_id</code>, <code>assay_name</code> and <code>seq_run_id</code> (see Table 2 and example datasets) to maintain clear links between files. For example, a curated ASV/OTU table should be named as otuFinal_<code>project_id</code>_<code>assay_name</code>_<code>seq_run_id</code> (e.g., otuFinal_gbr2022_MiFish_lib20230922.csv). If multiple data components are stored within a single spreadsheet workbook, the file name should be the <code>project_id</code> (e.g., gbr2022.xlsx). In this case, each worksheet name should follow the format in Table 2 without the <code>project_id</code> as it already appears in the main file name (e.g., otuFinal_MiFish_lib20230922). </p> <p>Table 1. Standardised names for each data type. In the examples, <code>project_id</code> = gbr2022, <code>samp_name</code> = S1, <code>assay_name</code> = eSERUS and MiFish for targeted and metabarcoding assay approaches respectively, <code>seq_run_id</code> = run20230922</p> Data type Name format Example Project metadata projectMetadata_<code>project_id</code> projectMetadata_gbr2022.csv Sample metadata sampleMetadata_<code>project_id</code> sampleMetadata_gbr2022.csv PCR standard data stdData_<code>project_id</code> stdData_gbr2022.csv eLow Quant data eLowQuantData_<code>project_id</code> eLowQuantData_gbr2022.csv Amplification data ampData_<code>project_id</code>_<code>assay_name</code> ampData_gbr2022_eSERUS.csv Experiment/run metadata experimentRunMetadata_<code>project_id</code> experimentRunMetadata_gbr2022.csv Non-curated ASV/OTU table otuRaw_<code>project_id</code>_<code>assay_name</code>_<code>seq_run_id</code> otuRaw_gbr2022_MiFish_run20230922.csv Curated ASV/OTU table otuFinal_<code>project_id</code>_<code>assay_name</code>_<code>seq_run_id</code> otuFinal_gbr2022_MiFish_run20230922.csv Non-curated Sequence/Taxa table taxaRaw_<code>project_id</code>_<code>assay_name</code>_<code>seq_run_id</code> taxaRaw_gbr2022_MiFish_run20230922.csv Curated Sequence/Taxa table taxaFinal_<code>project_id</code>_<code>assay_name</code>_<code>seq_run_id</code> taxaFinal_gbr2022_MiFish_run20230922.csv"},{"location":"guidelines.html#faire-metadata-checklist","title":"FAIRe metadata checklist","text":"<p>We developed a FAIR eDNA (FAIRe) metadata checklist, providing a comprehensive vocabulary for describing different data components and methodologies. Visit the Download tab to access the latest version of the checklist, the full template, and the previous versions with change logs. Thorough documentation is crucial for improving the transparency and reproducibility of studies, as well as for enabling the evaluation of data suitability for specific reuse cases. The FAIRe checklist consists of 337 data terms (38 mandatory, 51 highly recommended, 128 recommended and 120 optional terms), organised into workflow sections (e.g., sample collection, PCR, bioinformatics). Most of the mandatory, highly recommended and recommended information is generated naturally during the course of any project. As a result, data meeting the minimum reporting requirements can be readily made available and submitted to public repositories. </p>"},{"location":"guidelines.html#vocabulary-basis","title":"Vocabulary basis","text":"<p>The FAIRe metadata checklist consists of terms sourced from existing data standards such as MIxS, DwC, and the DNA-derived data extension to DwC. MIxS offers several checklists and extensions relevant for eDNA data, including Minimum Information about a Marker Sequence (MIMARKS), Minimum Information about Metagenome or Environmental Sequence (MIMS), and extensions for various environments (e.g., water, sediment, soil, air, host associated, and symbiont associated). These standards were reviewed and incorporated into the FAIRe metadata checklist where applicable to ensure comprehensive coverage of eDNA data needs. We have also incorporated terms from MIQE guidelines (Bustin et al., 2009), the single species eDNA assay development and validation checklist (Thalinger et al., 2021) and Minimum Information for eDNA and eRNA Metabarcoding (MIEM) guidelines (Klymus et al., 2024).  The source and URI for each term from existing standards are documented in the FAIRe metadata checklist under the columns \u2018source\u2019 and \u2018URI\u2019.  Where necessary, modifications have been made to term names, descriptions, and examples for easier interpretation by the eDNA community. A total of 158 new terms were proposed to accommodate some diverse attributes of eDNA procedures and datasets, and to improve the possibility of evaluating fitness for reuse. Among these are terms related to targeted assay detection workflows (e.g., <code>lod_method</code>, <code>std_seq</code>), bioinformatic tools, filtering parameters and cutoffs (e.g., <code>demux_tool</code>, <code>demux_max_mismatch</code>), taxonomic assignment metrics (e.g., <code>percent_match</code>), and taxon screening methods (i.e., <code>screen_contam_method</code>, <code>screen_nontarget_method</code>). The FAIRe checklist also includes a range of environmental variables relevant to eDNA samples, sourced from the MIxS sample extensions including Water, Soil, Sediment, Air, HostAssociated, MicrobialMatBiofilm and SymbiontAssociated. These variables are typically recorded during sampling events, and sharing them enables more advanced modelling approaches among other applications. If suitable term names are not available in the FAIRe checklist , users should search for them in existing standards, such as MIxS and DwC, and use these standardised terms where possible. If relevant terms cannot be found in these resources, users may add new terms using clear, concise, and descriptive names within related tables.</p>"},{"location":"guidelines.html#specified-use-of-terms-and-controlled-vocabularies","title":"Specified use of terms and controlled vocabularies","text":"<p>Several approaches were implemented to increase standardisation of terms. Firstly, for some free-text terms from existing standards, we propose using controlled vocabularies. For example, the value for the <code>target_gene</code> term should be selected from a list of 28 gene regions  used for barcoding. This standardisation prevents variations of COI such as CO1, COXI, COX1, Cytochrome oxidase I gene, Cytochrome c oxidase I, and spelling mistakes/variants of these from being entered. If a suitable value cannot be found among the specified values of the vocabulary, it is possible to enter \u201cother:\u201d followed by a free-text description. Secondly, units of numeric variables are strictly specified based on the International System of Units wherever possible, and only numeric entries without units are allowed. When units are necessary for unambiguous communication, separate terms for measurement value and measurement unit are provided, with the unit restricted to a controlled vocabulary (e.g., <code>samp_size</code> for the numeric value and <code>samp_size_unit</code> with options of mL, L, mg, g, kg, cm2, m2, cm3, m3, and other). Thirdly, formats for some data terms are restricted, generally following other standards (i.e., DwC, MIxS) and data infrastructures (i.e., GBIF, OBIS, INSDC). For example, <code>decimalLatitude</code> and <code>decimalLongitude</code> are required terms, and must be in decimal degrees using WGS84 datum. If records in an original datasheet follow other formats (e.g., degree minute second, UTM, datums other than WGS84), they should be stored under the terms <code>verbatimLatitude</code> and <code>verbatimLongitude</code> while <code>decimalLatitude</code> and <code>decimalLongitude</code> are also required to be entered. Similarly, sampling date and time are summarized under the term <code>eventDate</code>,  which utilises the ISO 8601 format, followed by the difference from UTC time (e.g., \"2008-01-23T19:23-06:00\" in the time zone six hours earlier than UTC). Original records of date and time before conversion into the ISO 8601 format should be stored under the terms <code>verbatimDate</code> and <code>verbatimTime</code>. </p>"},{"location":"guidelines.html#missing-values","title":"Missing values","text":"<p>Information may be missing for various reasons, and historically, science has used a variety of conventions to denote missing data. Furthermore, geographical coordinates can be generalised or withheld to protect endangered species or culturally important sites for Indigenous People and Local Communities (Chapman, 2020; Chapman and Wieczorek, 2020; Frank et al., 2015, https://fnigc.ca/). The reasons for withholding or generalising data should be described under the terms <code>informationWithheld</code> and <code>dataGeneralization</code> respectively in project metadata. When a value is missing from a mandatory term, it is required to provide the reason following the format of the INSDC missing value controlled vocabulary. It is also recommended to apply this approach for non-mandatory terms.  Below, we provide the complete list of missing value terms, including their full hierarchical structure for clarity. Only the bold texts should be used to describe the missing values (i.e., do not include the parenthetical explanations or add your own reasons). </p> <ul> <li>not applicable: control sample</li> <li>not applicable: sample group</li> <li>not applicable (other reasons not applicable)</li> <li>missing: not collected: synthetic construct</li> <li>missing: not collected: lab stock</li> <li>missing: not collected: third party data</li> <li>missing: not collected (other reasons not collected)</li> <li>missing: not provided: data agreement established pre-2023</li> <li>missing: not provided (other reasons not provided)</li> <li>missing: restricted access: endangered species</li> <li>missing: restricted access: human-identifiable</li> <li>missing: restricted access (other reasons restricted access)</li> </ul>"},{"location":"guidelines.html#data-submissionpublication","title":"Data submission/publication","text":"<p>Publishing eDNA datasets to databases such as GBIF, OBIS and INSDC, is an essential step in ensuring FAIR data practices. These platforms ensure data findability and accessibility for scientific research and decision-making that rely on open data, either through application programming interfaces (APIs) or sophisticated web-browser interfaces. They offer data validation procedures during submission, and provide additional standardisation to ensure interoperability. Persistent sample and sequence identifiers are provided upon the submission of data to these databases, which are essential for machine-readability and long-lasting reference to the data. For example, in GBIF, dataset authors and publishers receive credit through unique dataset Digital Object Identifiers (DOIs) that enable citation tracking and automatic usage reporting.  Since not all eDNA data components can be sufficiently accommodated in any single of these databases, they should be submitted in parts to the most suitable databases (Figure 2).  These are the current recommended practices for sharing FAIR eDNA data through the larger databases/infrastructures: </p> <ol> <li>Submit raw DNA sequence data and metadata to nucleotide databases such as INSDC (ENA, NCBI or DDBJ). For instance, NCBI allows submission of project metadata via BioProject, sample metadata via BioSample, and raw sequences via the Sequence Read Archive (SRA) (Barrett et al., 2012). This links metadata with sequence data, enabling users to retrieve complete datasets by querying BioProject or BioSample records.   </li> <li>Publish the derived biodiversity data \u2013 i.e., inferred taxon occurrences with sequences and metadata \u2013 to biodiversity databases such as GBIF, or GBIF and OBIS (if marine data). Both GBIF and OBIS rely on data being formatted according to the DwC Standard with the DNA-derived data extension as described in the co-authored guide (Abarenkov et al., 2023). The dataset is converted into so-called Darwin Core Archive (DwC-A) that can be indexed by GBIF and OBIS.  </li> <li>Archive the remaining data components (i.e., standard data for targeted assay studies and raw ASV/OTU table for metabarcoding studies), formatted according to the FAIRe guidelines, in open data repositories such as Dryad, Zenodo and Figshare or as supplementary materials in journals. This approach ensures comprehensive storage of raw data which are not addressed in the above steps. It is, however, essential to recognise that datasets in these repositories are not indexed to the extent required for effective searchability and interoperability within larger eDNA, biodiversity, or nucleotide data platforms.</li> <li>Publishing protocols and sharing bioinformatics and analytical codes is also highly recommended (Jenkins et al., 2023; Teytelman et al., 2016) through open-source platforms such as protocols.io and WorkflowHub. </li> <li>All above data, protocols and codes should be assigned DOIs, which should be documented in data accessibility statements within journal articles, as well as in the metadata terms such as <code>seq_archive</code>, <code>code_repo</code> and <code>associated_resource</code>.</li> </ol> <p>The guidelines provided here result in data formatted slightly different from those accepted by nucleotide and biodiversity databases. Hence, minor reformatting is required. One example is the necessary data format when multiple assays were applied in a single project. GBIF recently launched the Metabarcoding Data Toolkit (MDT), a user-friendly web application that reshapes tabular metabarcoding data\u2014similar to the format proposed here\u2014and publishes it to GBIF and OBIS complying with the guidelines in Abarenkov et al. (2023).  The GBIF MDT currently requires a separate input dataset for each assay (hence information from project and sample metadata are repeated), whereas the FAIRe checklist allow the metadata from multiple assays to be combined (Figure 5). An R script (FAIRe2MDT) has been developed to convert FAIRe data templates for GBIF and OBIS publication via MDT, bridging the differences in data formats between FAIRe and GBIF standards (see below section \u2018Available scripts and tools\u2019). Using these tools in combination streamlines the publication of eDNA data to GBIF and OBIS. Overall, there is currently a strong effort by biodiversity data platforms to improve the suitability of depositing eDNA-based data.</p>"},{"location":"guidelines.html#available-example-datasets-scripts-and-tools","title":"Available example datasets, scripts and tools","text":"<p>To provide eDNA practitioners with suggestions for how to format and publish their data, various example datasets are available in Supplementary Information (see Table S2 and S3 for the list of example datasets using targeted and metabarcoding assays, respectively). The below scripts and tools were developed to aid data formatting according to the guidelines, and are made available at the FAIRe github repositories. </p> <ul> <li>FAIRe-ator (FAIR eDNA template generator): This R function creates data templates based on user-specified parameters, such as assay type (i.e., targeted or metabarcoding assay), sample type (e.g., water, sediment), and the number of assays applied. Additionally, the function allows users to input project ID and assay name(s), which ensures correct file name formatting and pre-fills the <code>project_id</code> and <code>assay_name</code> terms in the template. Download the README and R code here.</li> <li>FAIReSheets (FAIR eDNA template generator for Google Sheets): This python script creates the FAIRe eDNA Data Template in Google Sheets. It replicates the template creation from the FAIRe-ator, except FAIReSheets outputs the template to Google Sheets rather than a Microsoft Excel spreadsheet. Download the README and Python code here.</li> <li>FAIRe-fier (FAIR eDNA metadata verifier): This tool is accessible via web interface, with additional metadata and citation details available at http://hdl.handle.net/102.100.100/706519?index=1. It allows users to validate their metadata without requiring any scripting knowledge. After filling their project and sample metadata, users can upload it to the tool for validation. The tool checks various components, including whether mandatory terms are completed or, if not, whether a valid reason has been provided under the <code>information_withheld</code> term in the project metadata. It also verifies that controlled vocabulary entries and fixed-format terms, such as <code>eventDate</code> (which must follow ISO 8601 format), are correctly formatted. If any issues are found, users receive output with warning and error messages, indicating where corrections are needed. Formatted output will be produced when there is no warning message.</li> <li>FAIRe2MDT: The R script to convert FAIR eDNA data templates for GBIF submission via MDT. Download the R code here.</li> </ul>"},{"location":"guidelines.html#useful-resources","title":"Useful resources","text":"<p>The current FAIRe frameworks were developed using the following resources as foundational starting points.</p> <ul> <li>Darwin Core Quick Reference Guide</li> <li>Minimum Information about any (x) Sequence (MIxS) standard</li> <li>Darwin Core extension of DNA derived data </li> <li>Abarenkov et al., (2023) Publishing DNA-derived data through biodiversity data platforms, v1.3. Copenhagen: GBIF Secretariat.</li> <li>The OBIS manual</li> <li>GBIF Metabarcoding Data Toolkit</li> </ul>"},{"location":"next.html","title":"What's next","text":"<p>While we believe the current guidelines will significantly empower users to enhance the FAIRness of eDNA data, other contributions will also make a difference. </p>"},{"location":"next.html#raising-awareness","title":"Raising awareness","text":"<p>Our next step focuses on raising awareness of FAIR principles, existing guidelines, and available tools to promote their adoption with the eDNA communities. To achieve this, the lead author will host workshops with eDNA practitioners over the coming year, offering hands-on opportunities to introduce FAIR data principles and provide practical guidance on implementing standardised formatting protocols. The first workshop took place at the eDNA conference in Wellington, New Zealand, in February 2025. More than a training exercise, these workshops are crucial for establishing direct communications, identifying communities\u2019 needs, and addressing challenges to adopting FAIR data practices within their workflows. This step is particularly important as open and FAIR data can only be achieved through eDNA practitioners\u2019 collaborative efforts, and rely on their commitment to making data accessible for broader use. </p> <p>Several studies have explored Open Science behaviours and identified the barriers among scientists (e.g., Norris and O\u2019Connor, 2019; Tenopir et al., 2015). These barriers include technical and resource limitations, such as lack of standards, tools, time and skills to navigate required data management systems (Tedersoo et al., 2021; Tenopir et al., 2015). Additionally, multiple perceived barriers exist, including unawareness of the value of data for others, fear of scrutiny due to potential mistakes, resistance to openly sharing data given large efforts invested to secure funding in the face of limited resource for scientific research and resistance to changing existing practices (Norris and O\u2019Connor, 2019; Tenopir et al., 2015). </p> <p>To address these challenges, it is essential to communicate the diverse benefits of data sharing beyond reproducibility and documentation. Apart from the obvious public good and enabling data reuse, data sharing leads to various benefits to data providers. These include increased visibility and citation of associated work (Colavizza et al., 2020; Piwowar et al., 2007; Wood-Charlson et al., 2022), direct citation metrics of data accessed through databases (e.g., GBIF), and enhanced collaborations and co-authorships, which all lead to improved professional stature (Bethlehem et al., 2022; McKiernan et al., 2016; Piwowar and Vision, 2013; Whitlock, 2011). Many eDNA practitioners, however, are not yet fully aware of these benefits. Establishing open and direct communications with them through workshops will help us identify their unique barriers and needs, work collaboratively to overcome them, and foster a culture of FAIR data practices in the community. </p>"},{"location":"next.html#revising-updating-and-integrating-the-guidelines","title":"Revising, updating and integrating the guidelines","text":"<p>As eDNA science is relatively new and rapidly evolving, new methods and technologies will emerge. To improve and adjust the FAIRe metadata checklist and formatting guidelines and keep them relevant, regular revisions and updates are essential. Input from data providers and reusers is vital to this process, and we welcome feedback at any time. For details on how to get involved, please see 'How to contribute'. Updated guidelines will be available at https://fair-edna.github.io.</p> <p>Leveraging the network of the co-authors (e.g., GSC, TDWG, ENA, GBIF, OBIS, ALA) we aim to continuously revise and co-develop the FAIRe metadata checklist to integrate with established data standards and databases (Figure 6). Implementation and alignment of FAIRe procedures and formats will be supported through contributions to the existing guidelines for publishing DNA-derived biodiversity data (Abarenkov et al., 2023), and development of scripts, like the FAIRe-ator and FAIRe2GBIF, and tools like the GBIF MDT. Integration of the FAIRe checklist into the GSC suite (i.e., MIxS), that will facilitate its adoption within the INSDC system, will be pursued through collaboration with the GSC MIxS Compliance and Interoperability Working Group (CIG) and with input from TDWG (Figure 6). </p> <p>In the scope of existing agreements between TDWG and GSC to collaborate on development of specifications for sequence-based biodiversity data, we will support the integration of the FAIRe checklist into the DwC DNA Derived Data extension. Continued revision, testing, and alignment with these organisations' objectives is needed to ensure successful implementation. Important milestones include the publication of updated guidelines for publishing DNA-derived biodiversity data (Abarenkov et al., 2023), publication of the eDNA MIxS checklist and publication of the FAIRe-aligned DwC DNA Derived Data extension. Achieving these milestones will serve as evidence that the FAIRe checklist is adequate and accepted by the eDNA community. Long-term maintenance would be ideally managed by these consortia and organisations, but will depend on sustained engagement and interest from the eDNA science community in the FAIRe checklist.</p> <p>We have developed the current guidelines with great interest to integrate them into journal publication requirements. We have collaborated with Wiley and Environmental DNA to develop a roadmap for integrating FAIR data principles into their publication procedures. This roadmap involves three key steps; 1) strongly recommending adherence to the FAIRe guidelines upon journal article publication, 2) making compliance mandatory in due course, and 3) extending these requirements to additional journals including Molecular Ecology and Ecology and Evolution. While most journals mandate data sharing, submitting data in a FAIR manner is not yet a widespread practice (Roche et al., 2015). Implementation of the FAIR data guidelines is a crucial and effective strategy given the large number of eDNA studies published each year with the tremendous volume of genetic and species occurrence data they generate (Takahashi et al., 2023). </p> Figure 6. Roadmap for continued development of the FAIRe checklist and guidelines, with time represented from left to right and the star representing the current point in time"},{"location":"next.html#harmonising-fair-and-care","title":"Harmonising FAIR and CARE","text":"<p>The CARE principles, which stand for Collective benefit, Authority to control, Responsibility and Ethics, were developed by the Global Indigenous Data Alliance to protect the Indigenous rights and interests in Indigenous data which include information, data, and traditional knowledge about their resources and environments (Carroll et al., 2021, 2020). The people- and purpose- oriented CARE principles are designed to complement the data-centric FAIR principles by ensuring that Indigenous data remain FAIR whilst centring Indigenous sovereignty (Mc Cartney et al., 2023). Implementing the CARE principles into FAIR data initiatives is essential to ensure that data use respects Indigenous rights, serves a meaningful purpose, and promotes wellbeing of Indigenous Peoples and Local Communities (IPLC) (Carroll et al., 2021; O\u2019Brien et al., 2024). </p> <p>Mc Cartney et al., (2023) established a roadmap for the lifecycle of eukaryotic biodiversity sequencing data following the CARE principles. It identifies actions required from researchers throughout each study to build sustainable partnerships with IPLC. In the context of FAIR initiatives, it is crucial to develop culturally aware linked metadata. For example, generalising or withholding culturally sensitive data, such as geographical coordinates of culturally important sites, is respected and accepted, which can be mapped under the terms dataGeneralization and informationWithheld in the FAIRe metadata checklist and other standards and databases (Darwin Core Maintenance Group, 2021; Chapman, 2020). Information on access, use permission, and the person or organisation owning and managing data rights, should be defined at the start of each project, and recorded under the terms accessRights and rightsHolder. Further works are needed in contextual metadata to document Traditional Knowledge Labels and Notices (Liggins et al., 2021; \u201cLocal Contexts \u2013 Grounding Indigenous Rights,\u201d 2023), provenance about place and people (Mc Cartney et al., 2023), and the cultural importance of species (Reyes-Garc\u00eda et al., 2023). Meetings, workshops and collaboration across various stakeholders including IPLC and eDNA communities are essential to recognise the benefits of eDNA data sharing in various aspects, exchange more information and raise discussions. All these efforts are critical to harmonising FAIR and CARE, establishing and retaining the link between people and nature, and serving as a vehicle for knowledge transfer, capacity development, two-way science and biodiversity conservation. </p> Image credit: https://www.gida-global.org/care"},{"location":"workinggroup.html","title":"Working group","text":""},{"location":"workinggroup.html#faire-working-group-members","title":"FAIRe Working Group members","text":"<ul> <li>Miwa Takahashi (0000-0001-8952-051X) (Corresponding author: miwa.takahashi@csiro.au)</li> <li>Oliver Berry (0000-0001-7545-5083)</li> <li>Tobias Guldberg Fr\u00f8slev (0000-0002-3530-013X)</li> <li>Joana Paup\u00e9rio (0000-0003-2569-0768)</li> <li>Bettina Thalinger (0000-0001-9315-8648)</li> <li>Katy Klymus (0000-0002-8843-6241)</li> <li>Cecillia Villacorta Rath (0000-0002-1060-5447)</li> <li>Caren C Helbing (0000-0002-8861-1070)</li> <li>Katherine Silliman (0000-0001-5964-3965)</li> <li>Luke R Thompson (0000-0002-3911-1280)</li> <li>Stephen Formel (0000-0001-7418-1244)</li> <li>Suk Yee Yong (0000-0002-5204-2902)</li> <li>Gareth Jenkins (0009-0000-2919-0723)</li> <li>Martin Laporte (0000-0002-0622-123X)</li> <li>Bruce Deagle (0000-0001-7651-3687)</li> <li>Sean P Jungbluth (0000-0001-9265-8341)</li> <li>Sachit Rajbhandari (0000-0002-9952-0801)</li> <li>Lynsey Harper (0000-0003-0923-1801)</li> <li>Nicholas Dunn (0000-0002-6231-5222)</li> <li>Thomas Stjernegaard Jeppesen (0000-0003-1691-239X)</li> <li>Andrew Bissett (0000-0001-7396-1484)</li> <li>Christopher Jerde (0000-0002-8074-3466)</li> <li>Lynn M Schriml (0000-0001-8910-9851)</li> <li>Christopher Hunter (0000-0002-1335-0881)</li> <li>Peter Woollard (0000-0002-7654-6902)</li> <li>Rachel Haderl\u00e9 (0009-0004-5752-3336)</li> <li>Katrina West (0000-0002-9026-5058)</li> <li>Erin Hahn (0000-0001-6459-9508)</li> <li>Peggy Newman (0000-0002-9084-5992)</li> <li>Shaun Wilkinson (0000-0002-7332-7931)</li> <li>Neha Acharya-Patel (0000-0002-3792-4675)</li> <li>Mark Louie D. Lopez (0000-0003-4288-4871)</li> <li>Guy Cochrane (0000-0001-7954-7057)</li> </ul>"}]}